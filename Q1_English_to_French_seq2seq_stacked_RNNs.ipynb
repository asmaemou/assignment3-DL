{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d76e0c",
   "metadata": {},
   "source": [
    "# Assignment 3 — Question 1: English → French Machine Translation (Stacked RNNs)\n",
    "\n",
    "This notebook gives you a clean, **ready-to-run** template for Q1. It builds and compares **stacked (2-layer) SimpleRNN, LSTM, and GRU** sequence-to-sequence models on the provided *Small_vocab_en* → *Small_vocab_fr* dataset.\n",
    "\n",
    "**What you’ll get out of this notebook**  \n",
    "- Reusable data pipeline (read → clean → tokenize → pad)  \n",
    "- Three seq2seq models: 2-layer SimpleRNN, 2-layer LSTM, 2-layer GRU  \n",
    "- Training with early stopping and LR scheduling  \n",
    "- Side-by-side metrics (loss, token accuracy, BLEU)  \n",
    "- **10 example translations from each model** ready to paste into your report  \n",
    "\n",
    "> ⚠️ Put your `Small_vocab_en` and `Small_vocab_fr` text files in a local folder (e.g., `./data/`).  \n",
    "> Each line should be one sentence. We’ll wrap the French targets with `<sos>` and `<eos>` tokens for decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & Imports\n",
    "import os, re, random, math, json, itertools, time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as KL, models as KM, callbacks as KC\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Optional: for BLEU\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "except Exception as e:\n",
    "    print(\"nltk not available; BLEU will be skipped until installed. You can run:\")\n",
    "    print(\"!pip install nltk && python -m nltk.downloader punkt\")\n",
    "    nltk = None\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4eabd4",
   "metadata": {},
   "source": [
    "## 1) Paths & Hyperparameters\n",
    "\n",
    "Update `DATA_DIR` if needed. You can start with small settings and scale up once it runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe71448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Configure paths ----\n",
    "DATA_DIR = \"./data\"           # put your text files here\n",
    "EN_FILE  = os.path.join(DATA_DIR, \"Small_vocab_en\")  # one sentence per line (English)\n",
    "FR_FILE  = os.path.join(DATA_DIR, \"Small_vocab_fr\")  # one sentence per line (French)\n",
    "\n",
    "# ---- Training hyperparameters ----\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "EMBED_DIM = 128\n",
    "UNITS = 256\n",
    "ENCODER_LAYERS = 2   # stacked depth\n",
    "DECODER_LAYERS = 2   # stacked depth\n",
    "DROPOUT = 0.2\n",
    "REC_DROPOUT = 0.2\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "# For speed during debugging, you can set MAX_SAMPLES (None to use all)\n",
    "MAX_SAMPLES = None   # e.g., 10000\n",
    "\n",
    "# Utility\n",
    "def exists_all(*paths):\n",
    "    return all(os.path.exists(p) for p in paths)\n",
    "\n",
    "print(\"Data present? \", exists_all(EN_FILE, FR_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818afdb",
   "metadata": {},
   "source": [
    "## 2) Load & Clean\n",
    "\n",
    "We lowercase and keep basic punctuation. French targets get `<sos>` and `<eos>` markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load raw text ----\n",
    "def load_lines(path, max_samples=None):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.strip() for ln in f]\n",
    "    if max_samples is not None:\n",
    "        lines = lines[:max_samples]\n",
    "    return lines\n",
    "\n",
    "assert exists_all(EN_FILE, FR_FILE), \"Missing dataset files. Please place Small_vocab_en and Small_vocab_fr under DATA_DIR.\"\n",
    "\n",
    "en_lines = load_lines(EN_FILE, MAX_SAMPLES)\n",
    "fr_lines = load_lines(FR_FILE, MAX_SAMPLES)\n",
    "assert len(en_lines) == len(fr_lines), \"Mismatch between EN and FR line counts\"\n",
    "\n",
    "print(f\"Loaded {len(en_lines):,} sentence pairs\")\n",
    "\n",
    "# ---- Basic clean: lower-case, keep letters, digits, space and basic punctuation ----\n",
    "_keep_re = re.compile(r\"[^a-zA-Z0-9'?,.!\\s-]\", re.UNICODE)\n",
    "\n",
    "def basic_clean(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = _keep_re.sub('', s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "en_clean = [basic_clean(s) for s in en_lines]\n",
    "fr_clean = [basic_clean(s) for s in fr_lines]\n",
    "\n",
    "# Add <sos> and <eos> to French targets\n",
    "fr_clean = [f\"<sos> {s} <eos>\" for s in fr_clean]\n",
    "\n",
    "print(en_clean[0])\n",
    "print(fr_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f4fc6",
   "metadata": {},
   "source": [
    "## 3) Tokenize & Pad\n",
    "\n",
    "We use Keras `Tokenizer` to map words→ids and `pad_sequences` to align lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da33fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Tokenize ----\n",
    "en_tok = Tokenizer(filters='')  # keep punctuation we already curated\n",
    "fr_tok = Tokenizer(filters='')\n",
    "\n",
    "en_tok.fit_on_texts(en_clean)\n",
    "fr_tok.fit_on_texts(fr_clean)\n",
    "\n",
    "en_vocab_size = len(en_tok.word_index) + 1  # +1 for padding idx=0\n",
    "fr_vocab_size = len(fr_tok.word_index) + 1\n",
    "\n",
    "# Convert to integer sequences\n",
    "en_seqs = en_tok.texts_to_sequences(en_clean)\n",
    "fr_seqs = fr_tok.texts_to_sequences(fr_clean)\n",
    "\n",
    "# Determine max lengths\n",
    "max_len_en = max(len(s) for s in en_seqs)\n",
    "max_len_fr = max(len(s) for s in fr_seqs)\n",
    "\n",
    "print(\"Vocab sizes -> EN:{}, FR:{}\".format(en_vocab_size, fr_vocab_size))\n",
    "print(\"Max lens -> EN:{}, FR:{}\".format(max_len_en, max_len_fr))\n",
    "\n",
    "# Pad\n",
    "X = pad_sequences(en_seqs, maxlen=max_len_en, padding='post')\n",
    "Y = pad_sequences(fr_seqs, maxlen=max_len_fr, padding='post')\n",
    "\n",
    "# For sparse_categorical_crossentropy, Y needs a final dimension\n",
    "Y_expanded = np.expand_dims(Y, -1)  # shape: (N, T_fr, 1)\n",
    "\n",
    "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape, \"Y_expanded:\", Y_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8e150",
   "metadata": {},
   "source": [
    "## 4) Build Stacked Seq2Seq Models\n",
    "\n",
    "We implement a shared builder that picks `SimpleRNN`, `LSTM`, or `GRU`.  \n",
    "**Encoder:** Embedding → N stacked recurrent layers (last returns a vector).  \n",
    "**Decoder:** RepeatVector(T_fr) → N stacked recurrent layers (return sequences) → TimeDistributed(Dense(|V_fr|, softmax)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model Factory ----\n",
    "def stacked_seq2seq(layer_type: str,\n",
    "                    en_vocab: int, fr_vocab: int,\n",
    "                    max_len_en: int, max_len_fr: int,\n",
    "                    embed_dim: int = 128, units: int = 256,\n",
    "                    enc_layers: int = 2, dec_layers: int = 2,\n",
    "                    dropout: float = 0.2, rec_dropout: float = 0.2):\n",
    "    assert layer_type in {\"SimpleRNN\", \"LSTM\", \"GRU\"}\n",
    "    RNN = getattr(KL, layer_type)\n",
    "\n",
    "    # Encoder\n",
    "    en_in = KL.Input(shape=(max_len_en,), name=f\"{layer_type}_encoder_input\")\n",
    "    x = KL.Embedding(en_vocab, embed_dim, mask_zero=True, name=f\"{layer_type}_src_embed\")(en_in)\n",
    "    # N-1 layers with return_sequences=True, last with return_sequences=False\n",
    "    for i in range(enc_layers - 1):\n",
    "        x = RNN(units, return_sequences=True, dropout=dropout, recurrent_dropout=rec_dropout,\n",
    "                name=f\"{layer_type}_enc_{i+1}\")(x)\n",
    "    x = RNN(units, return_sequences=False, dropout=dropout, recurrent_dropout=rec_dropout,\n",
    "            name=f\"{layer_type}_enc_{enc_layers}\")(x)\n",
    "\n",
    "    # Bridge\n",
    "    x = KL.RepeatVector(max_len_fr, name=f\"{layer_type}_repeat\")(x)\n",
    "\n",
    "    # Decoder (all return sequences)\n",
    "    for j in range(dec_layers):\n",
    "        x = RNN(units, return_sequences=True, dropout=dropout, recurrent_dropout=rec_dropout,\n",
    "                name=f\"{layer_type}_dec_{j+1}\")(x)\n",
    "\n",
    "    out = KL.TimeDistributed(KL.Dense(fr_vocab, activation='softmax'), name=f\"{layer_type}_classifier\")(x)\n",
    "\n",
    "    model = KM.Model(en_in, out, name=f\"Stacked_{layer_type}_Seq2Seq\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='tok_acc')])\n",
    "    return model\n",
    "\n",
    "rnn_model = stacked_seq2seq(\"SimpleRNN\", en_vocab_size, fr_vocab_size, max_len_en, max_len_fr,\n",
    "                            EMBED_DIM, UNITS, ENCODER_LAYERS, DECODER_LAYERS, DROPOUT, REC_DROPOUT)\n",
    "lstm_model = stacked_seq2seq(\"LSTM\", en_vocab_size, fr_vocab_size, max_len_en, max_len_fr,\n",
    "                             EMBED_DIM, UNITS, ENCODER_LAYERS, DECODER_LAYERS, DROPOUT, REC_DROPOUT)\n",
    "gru_model = stacked_seq2seq(\"GRU\", en_vocab_size, fr_vocab_size, max_len_en, max_len_fr,\n",
    "                            EMBED_DIM, UNITS, ENCODER_LAYERS, DECODER_LAYERS, DROPOUT, REC_DROPOUT)\n",
    "\n",
    "for m in [rnn_model, lstm_model, gru_model]:\n",
    "    m.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f701746",
   "metadata": {},
   "source": [
    "## 5) Train All Three Models\n",
    "\n",
    "We use the same hyperparameters across models for a fair comparison and employ early stopping + LR scheduling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c220a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Callbacks ----\n",
    "cbs = [\n",
    "    KC.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    KC.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n",
    "]\n",
    "\n",
    "history = {}\n",
    "for name, model in [(\"rnn\", rnn_model), (\"lstm\", lstm_model), (\"gru\", gru_model)]:\n",
    "    print(f\"\\n=== Training {name.upper()} ===\\n\")\n",
    "    hist = model.fit(\n",
    "        X, Y_expanded,\n",
    "        validation_split=VAL_SPLIT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True,\n",
    "        verbose=2,\n",
    "        callbacks=cbs\n",
    "    )\n",
    "    history[name] = hist.history\n",
    "\n",
    "# Save histories for later plotting\n",
    "with open(\"training_history_q1.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fbcf41",
   "metadata": {},
   "source": [
    "## 6) Decode Predictions & Evaluate\n",
    "\n",
    "We perform greedy decoding (argmax per timestep), strip `<sos>/<eos>`, and compute BLEU if `nltk` is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Utilities to decode ----\n",
    "id2fr = {idx: w for w, idx in fr_tok.word_index.items()}\n",
    "id2en = {idx: w for w, idx in en_tok.word_index.items()}\n",
    "\n",
    "def seqs_to_texts_id2tok(seqs, id2tok):\n",
    "    texts = []\n",
    "    for s in seqs:\n",
    "        toks = [id2tok.get(i, '') for i in s if i != 0]\n",
    "        texts.append(' '.join([t for t in toks if t]))\n",
    "    return texts\n",
    "\n",
    "def greedy_decode(model, X_in, max_len_fr, id2fr):\n",
    "    # Predict probs -> ids\n",
    "    probs = model.predict(X_in, verbose=0)\n",
    "    ids = probs.argmax(-1)\n",
    "    texts = []\n",
    "    for s in ids:\n",
    "        toks = []\n",
    "        for i in s:\n",
    "            w = id2fr.get(i, '')\n",
    "            if w == '<sos>':\n",
    "                continue\n",
    "            if w == '<eos>' or w == '':\n",
    "                break\n",
    "            toks.append(w)\n",
    "        texts.append(' '.join(toks))\n",
    "    return texts\n",
    "\n",
    "# Build references (without <sos>/<eos>)\n",
    "refs = []\n",
    "for s in Y:\n",
    "    toks = [id2fr.get(i, '') for i in s if i not in (0,)]\n",
    "    # remove sos/eos\n",
    "    toks = [t for t in toks if t not in ('<sos>', '<eos>', '')]\n",
    "    refs.append([toks])  # nested list for corpus_bleu\n",
    "\n",
    "def evaluate_model(model, name: str, sample_k: int = 10):\n",
    "    preds = greedy_decode(model, X, max_len_fr, id2fr)\n",
    "    # Compute BLEU if nltk is present\n",
    "    bleu2 = bleu4 = None\n",
    "    if nltk is not None:\n",
    "        sf = SmoothingFunction().method3\n",
    "        cand_tokens = [p.split() for p in preds]\n",
    "        bleu2 = corpus_bleu(refs, cand_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=sf)\n",
    "        bleu4 = corpus_bleu(refs, cand_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=sf)\n",
    "\n",
    "    # Show samples\n",
    "    idxs = np.random.choice(len(X), size=min(sample_k, len(X)), replace=False)\n",
    "    print(f\"\\n---- {name.upper()} SAMPLE TRANSLATIONS ----\\n\")\n",
    "    for i in idxs:\n",
    "        en_txt = ' '.join([id2en.get(t, '') for t in X[i] if t != 0])\n",
    "        # reference (detokenized)\n",
    "        ref_txt = ' '.join([id2fr.get(t, '') for t in Y[i] if t not in (0,)])\n",
    "        ref_txt = ref_txt.replace('<sos> ', '').replace(' <eos>', '')\n",
    "        print(\"EN:\", en_txt)\n",
    "        print(\"GT:\", ref_txt)\n",
    "        print(\"PR:\", preds[i])\n",
    "        print('-'*80)\n",
    "\n",
    "    return {\"bleu2\": bleu2, \"bleu4\": bleu4}\n",
    "\n",
    "scores = {}\n",
    "for name, model in [(\"rnn\", rnn_model), (\"lstm\", lstm_model), (\"gru\", gru_model)]:\n",
    "    scores[name] = evaluate_model(model, name, sample_k=10)\n",
    "\n",
    "print(\"\\nBLEU summary:\")\n",
    "print(json.dumps(scores, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff90c0a",
   "metadata": {},
   "source": [
    "## 7) Plot Training Curves\n",
    "\n",
    "Loss and token-level accuracy (on the teacher-forced target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5083dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"training_history_q1.json\", \"r\") as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "def plot_metric(metric='loss'):\n",
    "    plt.figure()\n",
    "    for name, hist in history.items():\n",
    "        plt.plot(hist[metric], label=f\"{name}-{metric}\")\n",
    "        if f\"val_{metric}\" in hist:\n",
    "            plt.plot(hist[f\"val_{metric}\"], linestyle='--', label=f\"{name}-val_{metric}\")\n",
    "    plt.title(metric)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_metric('loss')\n",
    "plot_metric('tok_acc')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
